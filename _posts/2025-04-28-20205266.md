---
layout: distill
title: AI810 Blog Post (20205266)
description: In this blogpost, I review two recent papers related to generative modeling in a viewpoint of a RL researcher.
date: 2025-04-28
future: true
htmlwidgets: true
#hidden: false

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Yongsik Lee
    affiliations:
      name: KAIST

# must be the exact same name as your blogpost
bibliography: 2025-04-28-20205266.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Introduction
  - name: Regulatory DNA Sequence Design with Reinforcement Learning  
    subsections:
    - name: Biological Bacgkround
    - name: Overview
    - name: Method
    - name: Experiment
    - name: "Personal Analysis and Commentary"
  - name: "Generator Matching: Generative Modeling with Arbitrary Markov Processes"
    subsections:
    - name: Overview
    - name: "Part 1: Generative Modeling"
    - name: "Part 2: Generator"
    - name: "Part 3: Generator Matching"
    - name: Unified View
    - name: Experiment
    - name: "Personal Analysis and Commentary"
  - name: Conclusion

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---


## Introduction

This blogpost reviews two recent papers: ‚ÄúRegulatory DNA Sequence Design with Reinforcement Learning‚Äù and ‚ÄúGenerator Matching: Generative Modeling with Arbitrary Markov Processes.‚Äù
While they are thematically distinct as the former addresses biological sequence design and the latter focuses on a unified framework of generative modeling, they are adjacent as both tackles the problem of generative processes. 
The first paper explores how reinforcement learning (RL) can improve the optimization of cis-regulatory DNA sequences, a crucial problem in synthetic biology. The second paper introduces a unifying mathematical framework for generative modeling through the lens of parameterized Markov processes.
Furthermore, as an RL researcher, I provide my viewpoint of these papers in RL perspective.


## Regulatory DNA Sequence Design with Reinforcement Learning (TACO) <d-cite key="yang2025regulatory"></d-cite> 

> This paper presents TACO, a generative method that leverages RL to fine-tune a pre-trained autoregressive model and also incorporates biological priors into the reward to design CREs with high fitness and diversity.   
**Revivew Outline**: This paper addresses the problem of DNA sequence design, which requires some understanding of biological concepts. As these may be unfamiliar to us, I begin with a brief introduction to them. Then I present the core content of the paper. Finally, I offer my detailed criticism and personal commentary.


### Biological Background

As we come from an AI background, I first provide a brief summary of relevant biological concepts appearing in this paper, based on my own understanding with external sources.

{% include figure.html path="assets/img/2025-04-28-20205266/TACO-biology.png" class="img-fluid" %}
<div class="caption">
    Conceptual flow of gene expression control.
</div>
 
 * **DNA**: a sequence of nucleotides (A, T, C, and G) that encodes genetic information.
   * **Gene**: a sub-sequence of DNA. Genes are not always active. **Gene expression** is the process that encodes a gene's information into proteins.
   * **Cis-regulatory elements (CRE)**: a non-coding sub-sequence of DNA that acts as on/off switch to control gene expression. A **promoter** determines when and where a gene is activated, and an **enhancer** boosts the level of gene expression. The ability of a CRE to modulate gene expression is referred to as its **fitness**.
     * **Transcription Factor Binding Sites (TFBS)**: a short sequence motif within a CRE. 
 * **Transcription Factor (TF)**: a protein that recognizes and binds to a specific TFBS. This binding influences a CRE's regulation of gene expression. An **activator** TF promotes gene transcription and expression, while a **repressor** TF hinders them.

In summary, a TF binding to a CRE via its TFBS modulates gene expression and each gene is regulated by distinct TFs and CREs.


### Overview

#### üìå Motivation

CREs play an essential role in regulating gene expression in a cell-type-specific manner. While millions of putative CREs have been identified over the past decade, most are naturally evolved and cover only a small region of the possible sequence space. Therefore, the design of synthetic CREs with _desired fitness_ is a promising direction, with broad applications across diverse domains.

The design of high-fitness CREs has primarily relied on _directed evolution_, which iteratively mutates and selects sequences in wet-lab settings. More recently, _fitness prediction models_ have been utilized as reward models to guide CRE optimization. However, current methods suffer from two limiations:
* Although the sequence space is large, they rely on local modifcation of existing or random sequences with iterative optimization, resulting in _local optima_ and _low diversity_.
* They generally do not utilize _biological prior knowledge_.
  

#### üìå Key Contributions

This paper proposes **TACO** (**T**FBS-**A**ware **C**is-regulatory element **O**ptimization), a RL fine-tuning method for a pre-trained autoregressive (AR) DNA model incorporating biological priors of TFBS information to improve CRE optimization. The key contributions are:

- **RL Fine-tuning for Pre-trained AR DNA Generative Models**: The suggested paradigm enables the generation of sequences with significantly higher diversity while also exploring those with superior functional performance.
- **Biologically-informed Prior Guided TFBS Reward**: The authors discover that using only TFBS frequency features of a CRE sequence can achieve high performance on CRE fitness prediction tasks. Moreover, the potential contribution of each TFBS is inferred via SHAP value and implemented as additional rewards.
- **Generation of high fitness and diversity Cell-type specific CREs**: TACO is evaluated under different optimization settings (active learning and offline model-based optimization) on real-world datasets and demostrated its effectiveness.


### Method

#### üìå Problem Formulation

{% include figure.html path="assets/img/2025-04-28-20205266/TACO-method.png" class="img-fluid" %}
<div class="caption">
    Overview of TACO, illustrating AR generation of a DNA sequence (BOS represents the beginning of the sequence).
</div>

A DNA sequence $$ X = ( x_1, \cdots, x_L ) $$ is defined as a sequence of nUcleotides, where $$ x_i \in \{A, C, G, T \} $$ represents the nucleotide at the $$ i $$-th position. The sequence has length $$ L $$. A large-scale dataset of CRE sequences with fitness measurements, $$ D = \{ (X_1, f (X_1)), \cdots, (X_N, f (X_N)) \} $$, is available. $$ D_\text{low} $$ is a subset of low-fitness sequences. In RL framework, the sequence generation is formulated as a _Markov Decision Process (MDP)_:
- **State**: $$ s_i $$, a partially generated DNA sequences up to time step $$ i $$
- **Action**: $$ a_i \in \{A, C, G, T \} $$, the next nucleotide at position $$ i $$
- **Policy**: $$ \pi_\theta $$, the AR generative model
- **Reward**:
$$
r(s_{i-1}, a_i) = 
\begin{cases}
r_{\text{fitness}}, & \text{if } i = L \\
r_{\text{TFBS}}(t), & \text{if } a_i \text{ results in a TFBS } t \in T \\
0, & \text{otherwise}
\end{cases}
$$

The generation process is illustrated in the above figure. The process terminates when the sequence length reaches $L$ and $$r_{\text{fitness}}$$ is given by the reward model. Whenever a TFBS $$T = \{t_1, t_2, \cdots, t_n\}$$ is identified, a positive (or negative) reward $$r_{\text{TFBS}}(t)$$ is given for generating activating (or repressive) TFBS.


#### üìå Step 1: Pre-training CRE-specific AR Model

In the pre-training stage, HyenaDNA <d-cite key="nguyen2023hyenadna"></d-cite> is adapted for the AR model by continual training on $$D_{\text{low}}$$. As HyenaDNA is trained on the entire human genome, continual pre-training is performed for CRE-specific regulatory patterns. Furthermore, pre-training on $$D_{\text{low}}$$ helps the policy generate sequences that resemble the true CRE distribution <d-cite key="jin2020multi"></d-cite><d-cite key="chen2021molecule"></d-cite>, offering a good starting point for RL fine-tuning.
The objective is to minimize:

$$
\min_{\theta} \mathbb{E}_{x \sim D_{\text{low}}} \left[ \sum_{i=1}^{L} -\log \pi_{\theta}(a_i \mid a_1, \cdots, a_{i-1}) \right]
$$


#### üìå Step 2: RL Fine-tuning for AR DNA Models

With the aforementioned MDP formulation, the objective in RL fine-tuning stage is to maximize the expected cumulative rewards: 

$$
\max_{\theta} J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{i=1}^{L} r(s_{i-1}, a_i) \right]
$$

REINFORCE <d-cite key="williams1992simple"></d-cite> is used to train the policy, and a hill climbing replay buffer and entropy regularization are utilized as auxiliary techniques following prior works <d-cite key="blaschke2020reinvent"></d-cite><d-cite key="ghugaresearching"></d-cite> to balance exploration and exploitation, thus improving performance.


#### üìå Inference of TFBS Regulatory Roles (Integrating Biological Prior)

{% include figure.html path="assets/img/2025-04-28-20205266/TACO-method2.png" class="img-fluid" %}
<div class="caption">
    Inference of TFBS Reward $r_{\text{TFBS}}(t)$
</div>

Firstly, construct the feature vector $$ h(X) = [h_1(X), h_2(X), \cdots, h_n(X)] $$ where $$ h_j(X) $$ denotes the frequency of the TFBS $$t_j$$ in sequence $$ X $$. LightGBM <d-cite key="ke2017lightgbm"></d-cite>, a decision-tree model, is trained to predict CRE fitness with this feature. 

Then the contribution of each TFBS frequency feature $$ h_j(X) $$ to the fitness prediction of the LightGBM model is inferred using SHAP values <d-cite key="lundberg2017unified"></d-cite> <d-footnote>SHAP values are a theoretically grounded method to estimate the contribution of each feature to the prediction of a model.</d-footnote>. The SHAP value $$ \phi_j(X) $$ for $$j$$-th TFBS $$t_j$$ in sequence $$ X $$ is:

$$
\phi_j(X) = \sum_{S \subseteq \{1, \dots, n\} \setminus \{j\}} \frac{|S|!(n - |S| - 1)!}{n!} \left( \hat{f}(S \cup \{j\}) - \hat{f}(S) \right)
$$

where $$ S $$ is a subset of features not containing $$ j $$ and $$ \hat{f} $$ is the model prediction. Then the TFBS reward $$ r_{\text{TFBS}}(t) $$ is computed as:

$$
r_{\text{TFBS}}(t) =
\begin{cases}
\alpha \cdot \mu_\phi(t), & \text{if } p\text{-value} < 0.05 \\
0, & \text{otherwise}
\end{cases}
$$

where $$ \alpha $$ is a hyperparameter and $$ \mu_\phi(t) $$ is the mean SHAP value of TFBS $$ t $$ across the dataset. <d-footnote> By assigning rewards only when p-value is less than 0.05, only statistically significant TFBSs contribute to the reawrds. </d-footnote> These rewards are incorporated into the RL fine-tuning to encourage activator TFBSs and discourage repressive TFBSs.


### Results & Analysis

#### üìå Experimental Setup

The experiments are conducted on two datasets: **yeast promoter**, which includes two types of growth media (_complex_ and _defined_) with DNA sequence length 80, and **human enhancer**, which consists of three cell lines (_HepG2_, _K562_, and _SK-N-SH_) with sequence length 200. MPRAs <d-cite key="sharon2012inferring"></d-cite> were employed to obtain all paired CRE sequences and their corresponding fitness measurements.

Three evaluation metrics are used. _Top_ is the mean fitness of highest-performaing 16 sequences from the optimized set $$ \mathcal{X}^* = \{X_1, \ldots, X_K\} $$ of 256 sequences. <d-footnote> In each optimization round, $K=256$ sequences are generated. </d-footnote> Both _Medium_ and _Diversity_ are computed using the highest-performing 128 sequences from the set of 256 sequences. _Medium_ is the median fitness among 128 sequences, and _Diversity_ is the median pairwise distance between all pairs within 128 sequences.

For baselines, FLEXS <d-cite key="sinai2020adalead"></d-cite> <d-footnote> FLEXS is a benchmark introduced in AdaLead paper. The FLEXS baseline in this paper corresponds to the Bayesian optimization implementation from that benchmark. </d-footnote>, AdaLead <d-cite key="sinai2020adalead"></d-cite>, PEX <d-cite key="anand2022protein"></d-cite> and CMAES <d-cite key="auger2012tutorial"></d-cite> are based on traditional optimization methods. DNARL is the modified version of the SOTA protein optimization method <d-cite key="lee2024robust"></d-cite> for CRE optimization.



#### üìå Setting 1: Active Learning

The oracle trained on the entire dataset $$D$$ is available in this setting, and employed as a reward model. <d-footnote> The oracle is employed for RL fine-tuning (as a reward model) and for evaluation. </d-footnote> Easy, middle, and hard subsets are constructed for each datset according to fitness values. These subsets are used as $$D_{\text{low}}$$ to pre-train the AR model for each difficulty.

{% include figure.html path="assets/img/2025-04-28-20205266/TACO-exp1.png" class="img-fluid" %}
<div class="caption">
    Results on human enhancer dataset in activate learning setting
</div>

The above table shows the results on human enhancer dataset for final optimized set $$\mathcal{X}^*$$ after 100 optimization rounds. TACO achieves both high fitness and diversity compared to the baselines. Similar results are obtained on yeast promoters dataset for each difficulty. <d-footnote> The results on yeast promoters dataset are omitted for clarity. </d-footnote>

{% include figure.html path="assets/img/2025-04-28-20205266/TACO-exp2.png" class="img-fluid" %}
<div class="caption">
    Evaluation metrics after each optimization round on human enhancer dataset
</div>

This figure shows the evaluation metrics after each optimization round. Among all methods, only TACO achieves both a stable increase in fitness and high diversity. Compared to other baselines, TACO's generative modeling paradigm enables this performance. 


#### üìå Setting 2: Offline Model-based Optimization (MBO)

In the previous active learning setting, we assumed the accessiblity to the oracle but what if we it is not available? In this offline MBO <d-cite key="reddy2024designing"></d-cite> <d-footnote> From the reference paper, the goal of an offline MBO algorithm is to produce designs which maximize an objective using a static dataset. The proxy objective function (surrogate in this paper) is trained, and the algorithms optimizes this proxy. The produced design is evaluated by the true objective. </d-footnote> setting  , the AR model is pre-trained on the complete dataset $$D$$ to simulate real-world scenarios in which sequences are rich but their fitness lables are unavailable. The _surrogate_ fitness prediction model is trained on $$D_{\text{offline}}$$, the smaller subset of $$D$$, and used during RL fine-tuning. The new metric _Emb Similarity_ is introduced to quantify the diversity of the final proposed sequences. <d-footnote> Emb Similarity is computed as the average pairwise cosine similarity of the embeddings of the proposed sequences. Here, the embeddings are obtained from the oracle. This metric can estimate the diversity of sequences in the latent feature space. </d-footnote>

{% include figure.html path="assets/img/2025-04-28-20205266/TACO-exp3.png" class="img-fluid" %}
<div class="caption">
    Results on human enhancer dataset (K562) in offline MBO setting
</div>

This table shows the results on K562 dataset. Two conditional generative models, regLM and DDSM are included as additional baselines. Due to unavailability of the oracle, the overall performance of each method decreases compared to the active learning setting. Again, TACO achieves both high fitness and diversity. Other methods are imbalanced between fitness and diversity, or are inferior to TACO.

#### üìå Ablation Study

{% include figure.html path="assets/img/2025-04-28-20205266/TACO-exp4.png" class="img-fluid" %}
<div class="caption">
    Results of the ablation study.
</div>

The above table demostrates the results of the abltaion study under offline MBO setting for two major components: pre-training and TFBS reward. For pre-training, 'w/o pretraining' (which uses a random initialization of the policy) achieves better performance for a particular metric in some cases, but are inferior for Medium metric. This indicates that pre-training helps the policy to start from a relatively reasonable exploration space. For TFBS reward, employing TFBS reward improves the Medium metric, indicating the prior-informed rewards facilitate efficient exploration of the policy in a more rational sequence space. When $$\alpha$$ increases from 0.01 to 0.1, overall performance degrades on HepG2, and Top and Medium metrics improves but Diversity worsens on K562 and SK-N-SH. This discrepancy would be due to the varying quality of TFBS reward across datasets.



### Personal Analysis and Commentary

In _Strengths_ and _Weaknesses, Limitations and Critique_, I present comments and evaluations on the paper in general viewpoint.
In _Opinion as an RL Researcher_, I share my thoughts from the viewpoint of a RL researcher.
_Thoughts on Future Research_ proposes several potential directions for future work inspired by this paper.
Lastly, I present what I personally gained from this paper in _Personal Takeaways_.

#### üìå Strengths

- **Novelty of TFBS Reward**: TACO incorporates TFBS motif information as biological prior knowledge into TFBS rewards, and is one of the first method leveraging such information in machine learning-based CRE design. Furthermore, the suggested TFBS reward is surrogate-agnostic.
- **Novelty of RL-based CRE design**: Unlike prior optimization-based methods, the authors introduced RL with pre-training AR for CRE, which provides a new paradigm in this domain.
- **Comprehensive Design of the Method**: The overall design of method is comprehensive, and each component is modularied and well-integrated. 
- **Empirical Validation**: The authors validated the effectiveness of TACO across different settings and datasets, where TACO achieves both high fitness and diversity compared to baselines. 

#### üìå Weaknesses, Limitations and Critique  

- **Presentation and Clarity**: There are several typos throughout the paper. And I found few parts are unclear to understand. The writing would be would be improved.
- **Missing Analysis on Emb Similarity**: While the authors introduced _Emb Similarity_ as an additional metric in offline MBO experiment, they do not provide explanation or justification on this metric. The Emb Similarity is very high compared to baselines, and this tendency is observed across datasets and difficulties as shown in the tables in appendix of the paper. This raises doubts whether TACO truly generate diverse sequences.
- **Limitation of TFBS Reward**: While the integration of biological priors as TFBS reward is novel, TFBS reward is only based on TFBS frequency and SHAP values. This simple structure does not consider other factors such as interactions between TFs and their orientation which can impact their regulatory roles <d-cite key="georgakopoulos2023transcription"></d-cite>. Moreover, the current method relied on a fixed TFBS databse, which may set an upper limit of the TFBS reward.
- **Necessity of RL Pipeline**: The overall pipeline is quite complex as it includes diverse factors such as AR pre-training, RL fine-tuning, and TFBS rewards with SHAP values. Morevover, auxiliary techniques, such as hill climing replay buffers, are employed. While TACO demonstrated its effectiveness, it is questionable whether the benefits of using this pipeline justify its complexity. Other baselines seems to be easier to implement and to utilize.
- **No Analysis on Computational Cost**: Related to the previous point, there is no analysis on computational cost. This would help to better understand TACO or to justify its use.
- **Dependency on Learned Reward Model**: As all rewards are obtained from learned reward models, the performance is hightly sensitive to the quality of these reward models.


#### üìå Opinion as an RL Researcher  

- The authors say that TACO is an RL method for CRE design. Let us revisit this.  
While TACO is not a complete offline RL method, it lacks the typical _environment_ in RL and interactions with the environment. In RL, an environment consists of its dynamics (a state transition function) and reward function. However, in TACO, there is no dynamics and policy itself just expands the sequence, i.e., the transition is deterministic. Only rewards are obtained from the separately trained reward models. Thus, we can view TACO as an RL-inspired optimization method.
- TACO overlaps with various areas of RL, while not perfectly aligned with any one of them. 
  - Model-based RL: Utilize learned reward models, but no dynamics.
  - Offline RL: Pre-train the policy with offline data, but also employ external reward models.
  - On-Policy and Off-Policy RL: TACO uses REINFORCE, an on-policy algorithm, while also employing a replay buffer is off-policy RL.
- The construction of TFBS reawrd is a case of _reward shaping_. Reward shaping provides a way to reflect the domain knowledge and to improve training. However, such shaping is difficult in general. If one wants to integrate further domain knowledge in the current TFBS reward, this process would not be trivial. Furthermore, the shaped reward can results in overfitting, which requires careful consideration.
- The authors noted that they employ REINFORCE as an RL fine-tuning algorithm following the previous work in molecule optimization. However, there are other advanced options like PPO. While REINFORCE is relatively simple and shows effectiveness with additional auxiliary techniques, using REINFORCE lacks justification.
- Nevertheless, this work is encouraing as RL techniques demonstrated the potential in real-world applications beyond typical RL simulation benchmarks.

#### üìå Thoughts on Future Work   

- Further development of TFBS reward would be interesting. As noted in the paper, exploring data-driven motif mining or explicitly incorporating factors such as interactions between TFs to model more complex TF activities could be promising directions.
- One can extend the pipeline of TACO to other protein design tasks and validate its generalization ability. Or, one can extend TACO to multi-objective (in addition to fitness) optimization.
- Validation in wet-lab setting over simulation-based evaluation would be important. But this is beyond the scope of AI researchers.
- RL techniques could be futher adjusted or newly adopted for TACO. For example, we can use PPO for RL fine-tuning or apply additional exploration strategies.
- We can take a hierarchical RL at the motif level. Or, we can introduce a curriculum RL approach.


#### üìå Personal Takeaways

TACO demonstrated the potential of RL as a promising tool in biology domain. Considering the growing interest in RL as a way to overcome limitations of traditional optimization algorithms, I believe cross-disciplinary collaboration with experts in other domains will become more important. It was also interesting that generative models can be framed in RL perspective. Combining diverse ingredients of RL to build effective and practical generative models is a direction I might explore in future work.


## Generator Matching: Generative Modeling with Arbitrary Markov Processes <d-cite key="holderrieth2025generator"></d-cite>

> This paper presents Generator Matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. This framework unifies current generative models such as diffusion models, flow matching.  
**Revivew Outline**: This paper follows a sequential structure, where each concept and section builds upon the previous one. This review follows the original flow, with partial reorganization for clarity. Part 1 introduces a view of generative modeling as a Markov process. Part 2 presents the core concepts necessary to construct the Generator Matching framework. Part 3 illustrates the Generator Matching and its extensions. A unified view of generative models and experimental results follow. Finally, I present my detailed criticism and commentary.  
**Note**: This work is mathematically dense. I focus on the core ideas and equations. Please refer to the paper for full mathematical details.


### Overview

#### üìå Motivation

Over the past decade, various generative models such as Diffusion models <d-cite key="ho2020denoising"></d-cite> and Flow models <d-cite key="lipman2022flow"></d-cite> have emerged. Despite their differences, many of these generative models share a common property: starting from easy-to-sample distribution, they iteratively contruct the sample $$X_{t+h}$$ of the next time step depeding only on the current state $$X_t$$, i.e., they are **Markov processes**.

Then, can we design a unified framework for these generative modeling methods?


#### üìå Key Contribution

- **Unified Framework for Generative Modeling**: This paper proposes **Generator Matching (GM)**, a framework for generative modeling with Markov processes on arbitrary state spaces. GM unifies prior generative modeling methods into a common framework that is modality-agnostic.
- **Model Combination**: GM facilitate combining models in two ways. Markov superpositions can construct ensembles of generative models. Multimodal generative models can be constructed by combining GM models built for single data modalities.
- **Universal Characterization and Novel Models**: This paper universally characterize the space of Markovian generative models on discrete and Euclidiean spaces. Moreover, jump models are introduces as an unexplored model class for $$\mathbb{R}^d$$.
- **Experiment**: On image and multimodal protein generation tasks, jump models and Markov superpositions achieve competitive results.

#### üìå Quick Glance at GM framework

{% include figure.html path="assets/img/2025-04-28-20205266/GM-overview.png" class="img-fluid" %}
<div class="caption">
    Generator Matching (GM) framework.
</div>

The above figure illustrates the GM framework to construct generative models. In GM framework, for the chosen state spaces and Markov processes, we find the conditional generator $$L_t^z$$ of the Markov process satisfying Kolmogorov Forward Equation (KFE). Then we train $$L_t^\theta$$ using the loss based on Bregman divergence. In this way, we can generate samples that approximate marginal data distribution. 


### Part 1: Generative Modeling

#### üìå Probability Path Approach

The goal of generative models is to generate novel samples $$z \sim p_{\text{data}}$$ when given samples $$x_1, \ldots, x_N \sim p_{\text{data}}$$ from a data distribution $$p_{\text{data}}$$ on a state space $$S$$. This can be done by transforming a simple distribution $$p_{\text{simple}}$$ into $$p_{\text{data}}$$ via probability paths. First sample $$z \sim p_{\text{data}}$$ and sample $$x \sim p_t(dx\vert z)$$ from **conditional probability path**, a set of time-varying probability distributions $$(p_t(dx\vert z))_{0 \leq t \leq 1}$$. Then we can construct **marginal probability path** which interpolates between $$p_{\text{simple}}$$ and $$p_{\text{data}}$$:

$$ z \sim p_{\text{data}}, \quad x \sim p_t(dx|z) \Rightarrow x \sim p_t(dx) $$

Now, the first design principle of GM follows:

> **Principle 1**  
> Given a data distribution $$p_{\text{data}}$$, choose $$p_{\text{simple}}$$ and a conditional probability path such that the marginal path $$(p_t)_{0 \leq t \leq 1}$$ fulfills $$p_0 = p_{_\text{simple}}$$ and $$p_1 = p_{\text{data}}$$.



#### üìå As a Markov Process

We can view generative models as **Markov Processes** <d-cite key="ethier2009markov"></d-cite>. What are Markov Processes? They satisfy the Markov property: the next state depends only on the current state. Mathematically, a time-continuous Markov process is $$(X_t)_{0 \leq t \leq 1}$$ for $$t \in [0, 1]$$ and a random variable $$X_t \in S$$, satisfying

$$
P[X_{t_{n+1}} \in A \mid X_{t_1}, X_{t_2}, \ldots, X_{t_n}] = P[X_{t_{n+1}} \in A \mid X_{t_n}]
$$

for all $$0 \leq t_1 < t_2 < \cdots < t_n < t_{n+1} \leq 1$$ and measurable set $$A \subseteq S$$. For each Markov Process, a corresponding **transition kernel** $$(k_{t+h \mid t})_{0 \leq t < t+h \leq 1}$$ assigns $$x \in S$$ a probability distribution $$ P[X_{t+h} \in A \mid X_t = x] = k_{t+h \mid t}(A \mid x) $$. 

With a Markov Process, we can generate data samples from $$p_{\text{data}}$$ following a marginal probability path $$(p_t(dx))_{0 \leq t \leq 1}$$: sample $$X_0 \sim p_0$$ and simulate $$X_{t+h} \sim k_{t+h \mid t}(\cdot \mid X_t)$$ step-wise up to $$t = 1$$. However, an abitrary general kernel $$k_{t+h \mid t}$$ is difficult to parameterize in a neural network. In diffusion models, the kernel can be closely approximated by simple parametric distributions such as Gaussians for small $$h>0$$ <d-cite key="sohl2015deep"></d-cite><d-cite key="ho2020denoising"></d-cite> . This idea can be extended to Markov processes, leading to the _generator_.


### Part 2: Generator

#### üìå Concept of Generator

Now, what is a generator? For $$k_{t+h \mid t}$$ with small $$h>0$$, below is an informal version of 1st-order Taylor approximation in $$t$$ with an error term $$o(h)$$.

$$
k_{t+h|t} = k_{t|t} + h L_t + o(h), \quad L_t := \left. \frac{d}{dh} \right|_{h=0} k_{t+h|t}, \quad k_{t|t}(\cdot|x) = \delta_x
$$

The first-order _linear_ approximation $$L_t$$ is called **generator** of the kernel $$k_{t+h \mid t}$$ <d-cite key="ethier2009markov"></d-cite><d-cite key="ruschendorf2016comparison"></d-cite>. The generator is the core concept of GM, and it will be shown that generative models (such as Diffusion) can be viewed as learning the generator of a Markov process.

The problem here is that a probability measure is not a standard function, thus the above equation is not well-defined. Hence, _test functions_ $$f : S \to \mathbb{R}$$ are employed to probe probability distributions and to define generator. The actions of marignal $$p_t$$ and kernel $$k_{t+h \mid t}$$ are defined for all $$f \in \mathcal{T}$$ with the linear function:

$$
\begin{aligned}
\langle p_t, f \rangle &\overset{\text{def}}{=} \int f(x)\, p_t(dx) = \mathbb{E}_{x \sim p_t}[f(x)] \quad \text{(Marginal Action)} \\
\langle k_{t+h\mid t}, f \rangle(x) &\overset{\text{def}}{=} \langle k_{t+h\mid t}(\cdot\mid x), f \rangle = \mathbb{E}[f(X_{t+h}) \mid X_t = x] \quad \text{(Transition Action)}
\end{aligned}
$$

Now we can define the generator with these actions.


#### üìå Definition of Generator

Consider the earlier _informal_ Taylor approximation. With test functions and the actions, we can take derivatives of $$\langle k_{t+h \mid t}, f \rangle(x)$$ per $$x \in S$$:

$$
\left. \frac{d}{dh} \right|_{h=0} \langle k_{t+h|t}, f \rangle(x) = \lim_{h \to 0} \frac{\langle k_{t+h|t}, f \rangle(x) - f(x)}{h} \overset{\text{def}}{=} [L_t f](x)
$$

This action is defined as the **generator** $$L_t$$. Moreover, the informal Taylor approximation is now well-defined as $$ \langle k_{t+h \mid t}, f \rangle = f + h L_t f + o(h) $$.

Under mild regularity assumptions, there is a unique correspondence between the Markov process and the generator <d-cite key="ethier2009markov"></d-cite><d-cite key="pazy2012semigroups"></d-cite>. Thus, we can parameterize a Markov process via a parameterized generator $$L_t^\theta$$. However, directly parameterizing a linear operator $$L_t$$ on function spaces with a neural network is difficult. Hence, restrict ourselves to certain subclasses of Markov processes <d-footnote> Flow matching restricts itself to generators $ L_t f = \nabla f(x)^\top u^\theta_t(x) $. </d-footnote>, and parameterize it linearly via a neural network. The following theorem characterizes generators.


> **Theorem 1 (Universal characterization of generators)**  
Under regularity assumptions  <d-footnote> Refer Appendix A.2 in the paper. </d-footnote>, the generators of a Markov processes $$X_t\ (0 \leq t \leq 1)$$ take the form:
> 1. **Discrete** $$(|S| < \infty)$$: 
The generator is given by a rate transition matrix $$Q_t$$ and the Markov process corresponds to a continuous-time Markov chain (CTMC).
> 2. **Euclidean space** $$(S = \mathbb{R}^d)$$: The generator has a representation as a sum  
> $$ L_t f(x) = \underbrace{\nabla f(x)^\top u_t(x)}_{\text{flow}} + \underbrace{\frac{1}{2} \nabla^2 f(x) \cdot \sigma_t^2(x)}_{\text{diffusion}} + \underbrace{\int [f(y) - f(x)] Q_t(dy; x)}_{\text{jump}} $$
> where $$u: [0,1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d$$ is a **velocity field**, $$\sigma: [0,1] \times \mathbb{R}^d \rightarrow S^{++}_d$$ is the **diffusion coefficient** ($$S^{++}_d = $$ positive semi-definite matrices), and $$Q_t(A\mid x)$$ is a finite measure called **jump measure**. $$\nabla^2 f(x)$$ denotes the Hessian of $$f$$ and $$\nabla^2 f(x) \cdot \sigma_t^2(x)$$ describes the Frobenius inner product.

Theorem 1 characterizes a wide class of Markov process models as well as the design space _exhaustively_ for $$S = \mathbb{R}^d$$ or $$S$$ discrete. Now we have the second principle:

> **Principle 2**  
> Parameterize a Markov process via a parameterized generator $$L_t^\theta$$. For $$S = \mathbb{R}^d$$, pameterize a Markov process (e.g., using a neural network) via a generator $$L_t$$ that is composed of (a subset of) velocity $$u_t$$, diffusion coefficient $$ \sigma_t^ 2$$, and jump measure $$ Q_t $$.

#### üìå Kolmogorov Forward Equation and Marginal Generator

The generator can not only parameterize a Markov Process, but also can check if a Markov process generates a desired probability path $$p_t$$. The latter is possible via **Kolmogorov Forward Equation (KFE)**

$$
\partial_t \langle p_t, f \rangle = \langle p_t, L_t f \rangle
$$

which is derived as
$$
\partial_t \langle p_t, f \rangle = \left.\frac{d}{dh} \right|_{h=0} \langle p_{t+h}, f \rangle = \langle p_t, L_t f \rangle
$$ <d-footnote> $\langle p_t, \cdot \rangle$ operation is linear to swap the derivative, and the $\langle p_t, \langle k_{t+h|t}, f \rangle \rangle = \langle p_{t+h}, f \rangle$.</d-footnote>
. KFE means, for a generator $$L_t$$ of a Markov process $$X_t$$, we are possible to reconstruct its marginal probabilities via their infinitesimal changes. Conversely, if a generator $$L_t$$ of a Markov process $$X_t$$ satisfies KFE, $$X_t$$ generates the probability path $$(p_t)_{0 \leq t \leq 1}$$ <d-cite key="rogers2000diffusions"></d-cite>.

Now, the key challenge of GM is **_Given a marginal probability path $$(p_t)_{0 \leq t \leq 1}$$, find a generator satisfying the KFE_**. However, the marginal path and a generator for the marginal path (**marginal generator**) are generally unknown or intractable. Let us assume we found a generator $$L_t^z$$ the generates $$p_t(\cdot \mid z)$$ for every $$z \in S$$. This $$L_t^z$$ is called **conditional generator**, and we construct marginal generator with $$L_t^z$$, i.e., generates a marginal path $$p_t$$ with conditional path $$p_t(\cdot \mid z)$$.


> **Proposition 1**  
> The marginal probability path $$(p_t)_{0 \leq t \leq 1}$$ is generated by a Markov process $$X_t$$ with generator
>
> $$ L_t f(x) = \mathbb{E}_{z \sim p_{1|t}(\cdot|x)} [L_t^z f(x)] $$
>
> where $$p_{1 \mid t}(dz \mid x)$$ is the posterior distribution (i.e., the conditional distribution over data $$z$$ given an observation $$x$$). For $$S = \mathbb{R}^d$$ and the representation in eq. **Theorem 1**, we get a marginal representation of $$L_t f(x)$$ given by:
>
> $$
\begin{aligned}
& \nabla f(x)^\top \mathbb{E}_{z \sim p_{1|t}(\cdot|x)} [u_t(x|z)] +
\frac{1}{2} \nabla^2 f(x) \cdot \mathbb{E}_{z \sim p_{1|t}(\cdot|x)} [\sigma_t^2(x|z)] \\
& + \int [f(y) - f(x)] \mathbb{E}_{z \sim p_{1|t}(\cdot|x)} [Q_t(dy;x|z)]
\end{aligned}
$$
>
> Generally, the above identity holds for any linear parameterization of the generator. <d-footnote> Refer Appendix A.6 in the paper. </d-footnote>

Following Proposition 1, we have the third principle:

> **Principle 3**    
> Derive a conditional generator $$L_t^z$$ satisfying the KFE for the conditional path $$p_t(\cdot|z)$$.


{% include figure.html path="assets/img/2025-04-28-20205266/GM-path.png" class="img-fluid" %}
<div class="caption">
    Markov models trained with different KFE solutions for the same probability path.
</div>


An important fact is that multiple Markov processes can follow the same probability path, satisfying the same KFE. The above figure illustrate different Markov models for the same probability path.<d-footnote> Refer to Example 1 and 2 in the paper. </d-footnote> For each probability path (Mixture or Geometric Average), each sample path is different. However, the histogram, i.e., the marginal paths are almost the same (MS denotes _markov superposition_ which will be explained in Extension 1 in Part 3).


### Part 3: Generator Matching

#### üìå Generator Matching Method

The final step is to approximate the _true_ marginal generator $$L_t$$ with a parameterized generator $$L_t^\theta$$. The 
$$L_t^\theta$$ is linearly parameterized by a neural network $F_t^\theta : S \times [0, 1] \rightarrow \Omega$ <d-footnote> $\Omega \subset V$ is a convex subset of some vector space $V$ with inner product $\langle \cdot, \cdot \rangle$.</d-footnote>in practice, and we train $$F_t^\theta$$ to approximate the ground-truth parameterization $$F_t : S \times [0, 1] \rightarrow \Omega$$ of $$L_t$$ (for instance, $$F_t = u_t$$ for flows).

**Bregman divergence** defined with a convex function $$\phi : \Omega \rightarrow \mathbb{R}$$, which is a general class of loss functions such as MSE or KL-divergence, is employed as a distance function to measure the discrepancy between $$F_t$$ and $$F_t^\theta$$:

$$
D(a, b) = \phi(a) - [\phi(b) + \langle a - b, \nabla \phi(b) \rangle], \quad a, b \in \Omega
$$

Then we can define **Generator Matching loss**:

$$
\mathcal{L}_{\text{gm}}(\theta) \overset{\text{def}}{=} \mathbb{E}_{t \sim \text{Unif},\ x \sim p_t} \left[ D(F_t(x), F_t^\theta(x)) \right]
$$

However, this objective is intractable because the marginal generator $$L_t$$ is unknown as mentioned earlier, and $$F_t$$ is unknown as well. 

Note that we know $$F_t^z, F_t^\theta, p_t(\cdot \mid z), D$$ and can sample $$z \sim p_{\text{data}}$$. Thus, for tractable and scalable training, define **conditional Generator Matching loss**:

$$
\mathcal{L}_{\text{cgm}}(\theta) \overset{\text{def}}{=} \mathbb{E}_{t \sim \text{Unif},\ z \sim p_{\text{data}},\ x \sim p_t(\cdot | z)} \left[ D(F_t^z(x), F_t^\theta(x)) \right]
$$

We can assume $$F_t(x) = \int F_t^z(x)\, p_{1\mid t}(dz \mid x)$$ from Proposition 1. Then the next proposition shows we can use CGM loss to optimize GM loss.

> **Proposition 2**  
> For any Bregman divergence, the GM loss and CGM loss have the same gradients as the CGM loss $$\mathcal{L}_{\text{cgm}}$$, i.e., $$ \nabla_\theta \mathcal{L}_{\text{gm}}(\theta) = \nabla_\theta \mathcal{L}_{\text{cgm}}(\theta) $$. Therefore, minimizing the CGM loss with Stochastic Gradient Descent will also minimizes the GM loss. Further, for this property to hold, $$D$$ must necessarily be a Bregman divergence.

Now we can learn $$L_t$$ with a scalable objective and _universally characterize the space of loss functions_. The last principle of GM follows:

> **Principle 4**  
> Train $$L_t^\theta$$ by minimizing the CGM loss with a Bregman divergence.


#### üìå Extension 1: Combining Models

As the generator is a linear operator and the KFE is a linear equation, we can combine generative models for the same state space.

> **Proposition 3 (Combining models)**  
> Let $$p_t$$ be a marginal probability path. Then the following generators solve the KFE for $$p_t$$ and consequently define a generative model with $$p_t$$ as marginal:
> 1. **Markov superposition:** $$\alpha_t^1 L_t + \alpha_t^2 L_t'$$, where $$L_t, L_t'$$ are two generators of Markov processes solving the KFE for $$p_t$$, and $$ \alpha_t^1, \alpha_t^2 \geq 0 $$ satisfy $$ \alpha_t^1 + \alpha_t^2 = 1 $$. We call this a **Markov superposition**.
> 2. **Divergence-free components:**
> $$ L_t + \beta_t L_t^{\text{div}} $$, where $$L_t^{\text{div}}$$ is a generator such that $$ \langle p_t, L_t^{\text{div}} f \rangle = 0 $$ for all $$ f \in \mathcal{T} $$, and $$\beta_t \geq 0$$. We call such $$L_t^{\text{div}}$$ **divergence-free**.
>
> 3. **Predictor-corrector:** $$ \alpha_t^1 L_t + \alpha_t^2 \bar{L}_t $$, where $$L_t$$ is a generator solving the KFE for $$p_t$$ in forward-time and $$\bar{L}_t$$ is a generator solving the KFE in backward time, and $$ \alpha_t^1, \alpha_t^2 \geq 0 $$ with $$\alpha_t^1 - \alpha_t^2 = 1$$.



#### üìå Extension 2: Multimodal and High-Dimensional Generative Modeling

Furthermore, we can combine generative models from two state spaces $$S_1$$, $$S_2$$ into the product space $$S_1 \times S_2$$. This provides two advantages: we can easily design a joint multimodal generative model easily, and can often reduce solving the KFE in high dimensions to the 1-dimensional case.

> **Proposition 4 (Multimodal generative models ‚Äì Informal version) <d-footnote> Refer to Appendix C.5 in the paper for the rigorous version. </d-footnote>**  
> Let $$q_t^1(\cdot | z_1)$$, $$q_t^2(\cdot | z_2)$$ be two conditional probability paths on state spaces $$S_1$$, $$S_2$$.  
> Define the conditional factorized path on $$S_1 \times S_2$$ as $$ p_t(\cdot | z_1, z_2) = q_t^1(\cdot | z_1)\ q_t^2(\cdot | z_2) $$. Let $$p_t(dx)$$ be its marginal path.
> 1. **Conditional generator:**  
>    To find a solution to the KFE for the conditional factorized path, we only have to find solutions to the KFE for each $$S_1, S_2$$. We can combine them component-wise.
> 2. **Marginal generator:**  The marginal generator of $$p_t(dx)$$ can be parameterized as follows: (1) parameterize a generator on each $$S_i$$ but make it values depend on all dimensions; (2) During sampling, update each component independently as one would do for each $$S_i$$ in the unimodal case.
> 3. **Loss function:** We can simply take the sum of loss functions for each $$S_i$$.


For example, consider a joint image-text generation with a joint flow and discrete Markov model with $$S_1 = \mathbb{R}^d$$, $$S_2 = \{1, \dots, N\}$$. A multimodal model can be constructed by making the vector field $$ u_t(x_t^1, x_t^2) \in \mathbb{R}^d $$ depending on both modalities $$x_t^1$$ and $$x_t^2$$, but updating the image modality as $$ X_{t+h}^1 = X_t^1 + h u_t(X_t^1, X_t^2) $$. The text modality is similarly updated depending on both $$(X_t^1, X_t^2)$$.



### Unified View

{% include figure.html path="assets/img/2025-04-28-20205266/GM-unified.png" class="img-fluid" %}
<div class="caption">
    Generative models in GM framework.
</div>

Now in the GM framework, we can vew genertaive models as methods to learn the generator of a Markov process. The above table shows the examples of important classes of markov processes (note that zero drift is assumed for diffusion). The core components such as generator, KFE, or CGM loss are listed for each Markov process. GM framework provided an unified view of various generative models.



### Experiment

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-20205266/GM-exp1.png" class="img-fluid" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-04-28-20205266/GM-exp2.png" class="img-fluid" %}
    </div>
</div>
<div class="caption">
    Experimental Results. The left table corresponds to aspects 1 and 2, while the right table corresponds to aspect 3.
</div>

As the design space of the GM framework is large and single classes of models (such as diffusion and flows) have already been optimized in prior works, the experiments focues on three aspects of GM.

#### üìå Aspect 1. Jump Models as a novel model class

The left table shows FID scores of image generation for CIFAR10 and ImagetNet32. <d-footnote>  Since jump models do not have an equivalent of classifier-free guidance yet, the experiment focus on unconditional generation. </d-footnote> Euler and 2nd order denote Euler sampling and ODE sampler, respectively. While the jump model underperforms compared to the current SOTA models, it shows promising results as a first version of an unexplored class of models.

#### üìå Aspect 2. Combining different model classes (Markov superposition)

Firstly, train a flow and jump model in the same architecture, and validate that the flow part adhives the SOTA performance as before. Then combine both models using a Markov superposition (MS in the left table denotes Markov superposition). Markov superposition significantly improves the performance for Euler sampling. Furthermore, combining 2nd order samplers for flow with Euler sampling for jumps in a _mixed_ sampling outperforms the SOTA flow models. This may futher enhanced by improving the jump model.

#### üìå Aspect 3. Multimodal State Spaces

Next, evaluate the GM's ability to design models for arbitrary and complex state spaces with protein generation tasks. Derive $$L_t^z$$ to the KFE on $$S=SO(3)$$ with a jump model. Then use Proposition 4 to make it multi-dimensional and combine it with a flow model on $$\mathbb{R}^d$$ and a discrete Markov model on $$\{1, \ldots, n\}^d$$ for $$n=20$$ (# amino acids). The state space becomes $$\mathbb{R}^d \times SO(3)^d \times \{1, \ldots, n\}^d$$. Without re-training or fine-tuning the pre-trained MultiFlow <d-cite key="campbell2024generative"></d-cite>, the SOTA model, _pseudo-marginalize_ the conditional jumps by predicting $$x \in SE(3)$$ and then taking a conditional setp with $$L_t^z$$. The right table shows the results, where the metrics Div and Nov represent diversity and novelty, respectively. Both sequence and structure spaces are sampled jointly in multimodal setting, while only the structure is sampled in unimodal setting. Including a jump model achieves SOTA results without re-training or fine-tuning MultiFlow.


### Personal Analysis and Commentary

As before, I present comments and evaluations on the paper from general viewpoint in _Strengths_ and _Weaknesses, Limitations and Critique_. In _Opinion as an RL Researcher_, I provide my thoughts in perspective of a RL researcher. 
Then I propose potential directions for future work inspired by this paper in _Thoughts on Future Research_.
Finally, I present what I personally gained from this paper in _Personal Takeaways_.


#### üìå Strengths  

I present the strengths of this paper including key contributions mentioned earlier:

- **A Unified Generative Modeling Framework**: GM is a unified framework of generative modeling via the concept of generator with Markov processes. This GM frameowrk is modality-agnostic and encompasses various existing generative models such as Diffusion models. While there have been pr works for unifying generative models, I believe this unified framework offers a comprehensive view for people working on generative models.

- **Theretical Foundation**: This work provides comprehensive and rigorous theoretical foundation, grounded in probabilistic process theory. The authors offers solid mathematical details, derivations, and proofs throughout the paper.

- **Recipes to Combine Models**: Using GM framework, we can combine different Markov processes and further extned to multimodal models. These recipes are useful and provide a way to construct diverse generative classes of Markov processes.

- **Novel Models**: This paper introduced a jump model based on jump processes. It takes a novel approach rather than prior flow or diffusion-based methods. While the authors does not highlight, I think the _pure diffusion_ model proposed in this paper is also novel and intersting.

- **Experimental Validation**: The authors validated the effectiveness of GM through several experiments. This can be addressed more comprehensively, but I think it is enough to support GM as the main contribution of this paper is theoretically-grounded unified framework.


#### üìå Weaknesses, Limitations and Critique  

While I believe this work is remarkable and provides strong contributions, I present some critique here.

- While I acknowledge the overall experimental setup and scope is enough as the main contribution is theoretically grounded framework, some experimental results are questionable. While the authors state the jump model shows promising results in the image generation task, it significantly underperforms baselines. 
- Related to the previous point, the jump model need more improvements. For example, sampling methods of jump models are less optimized. Moreover, its effectivenss and stability should be validated further.
- Practical aspects, sucha as the computational cost of learning and utilizing generators, need to be addressed.


#### üìå Opinion as an RL Researcher  

- A fundamental framework of RL is _Markov Decision Process_, which is also based on Markov property. GM unifies generative modeling via Markov Process. In RL perspective, this can be viewed as action-marginalized dynamics (transitions).

- Recently, generative models such as diffusion models are utilized as a planner like model-based RL <d-cite key="janner2022planning"></d-cite>. As shown in GM paper, the diffusion model is an example of GM. And we can combine differnt Markov processes in GM framework. When I became aware of this, I came up with the following idea:
  - Combine multiple diffsion-based planners to construct a multimodal planner
  - Or, combine planners with different or complementary inductive biases
  This could be a very intersting research direction.

- The Bregman divergence was also intersting. As it generalizes many losses inclduing MSE and KL divergence, I think it can be also employed for RL training objectives.


#### üìå Thoughts on Future Work   

- Investigating the pure diffusion models introduced in this paper, which learns a diffusion coefficient, would be an interseting direction.
- Further study of jump models on Euclidiean space could lead to a large class of generative models.
- We can dive into devlopment of better samplers for jump models.
- Exploring how GM framework and generators can be employed as a planner for RL would be a promising direction.
- As GM and jump models are evaluated in two domains, empirical investigation in other domains would be also intersting.



#### üìå Personal Takeaways

I'm not an expert in generative models. During reading this paper, I gained a comprehensive overview of recent generative models such as diffusion and flow models. The shared Markovian view between generative models and RL was also intersting. Furthermore, I got a promising research idea of combining models to construct RL planners.



## Conclusion

In this blogpost, I reviewed two papers on generative models. The first paper TACO leverages RL for CRE design, and the second paper provides a unified framework via generator of Markov processes for generative modeling. After addressing the key points of each paper, I presented an interpretation as an RL researcher.